# CMU深度增强学习&控制 第1讲：导论

## 目标

**行为\(Behavior\)**，是agent在一个特定目标下的一个动作序列。不同于在一个静态不变的环境里通过预先编程确定的行为（比如工业机器人大量使用的离线示教），**本课程学习如何在一个动态变化的环境里让agent学习行为。**静态环境里，执行固定行为往往就足够了。

## 监督学习

智能体（agent）在通过不断尝试进行学习时，需要对结果好坏有个评估，这就是“监督（supervision）”。一个在动态环境下学习特定行为的智能体，根据问题的不同一般会获得这几种监督：

**1.收益（reward）**。目标达成时获得的收益反馈，是一个标量数字。比如，比赛胜利，汽车没有事故，智能体正常运行等。

**2. 示教（demonstration）**。示范需要的行为是一条轨迹。比如，通过youtube烹饪视频教机械臂学习烹饪。

**3. 正确行为属性**。比如对于自动驾驶，好的行为属性包括保持在正确的车道内，与前车保持适当的车距等。

这三种监督提供的信息量从小到大，对应问题的难度也从困难到容易。

## 行为学习举例：跳高

**1.从收益中学习**：最早的跳高运动员们，没有老师教，只能根据跳的高度来调整动作。跳得越高收益越大。结果就是这些人花了许多年才从跨越式发展到背跃式。

**2. 从示教中学习**：运动员示范一次高级姿势的跳高，其他人学习这种标准姿势可以快速掌握跳得更高的技巧。

1. **从好的行为属性中学习**：在教练指导下的跳高学习。教练可以直接告诉初学者正确的行为属性，比如跳高时腿的姿势，跳高应采取的节奏等。无疑这种方式获得的信息量最大，也是学得最快的。

## 行为学习的特点

行为学习与其他机器学习范式（比如从图片中检测物体）有什么样的不同？以下分为四点来谈：

1. 智能体（agent）的**行为**会**影响**其在未来接收到的**数据；**

2. 不管行为的目标是否达到，**对其效果的衡量都将在很久以后才能获得；**

3. 在现实世界当中，行动都是需要时间来执行的，所以说这就**限制了**能够收集到的（能用于训练的）**样本的数量**。**电脑可以变得越来越快，而现实的物理世界却不能。**

几个例子：

* 超大规模（Supersizing）自我监督学习：论文的关键点在于它通过很长的时间（50K次尝试，700 robot hours）来收集足够多的数据用于机器人的行为学习，“Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours”

* 谷歌机器人农场：Google用几十台相同的工业机器人连续工作几个月，采集了超过80000组抓取数据。“Deep Learning for Robots: Learning from Large-Scale Interaction”

> 值得注意的是在这两个例子中，行为的结果都是很容易评估的，只需看观察物体是否成功抓取。如果结果很难评估，那么问题就非常难用增强学习解决。

```
4 行为的组合难以学习。这就是为什么在目前机器人RL大多数工作中只学习某个特定的行为，而不是较复杂的复合行为。学习组合行为远比学习组合的音频、图像难度更大。后两者已经使用深度学习取得了很好的效果。
```

* 下面是在医学研究下，人类婴儿学习行为的特点，或许对我们有参考意义：
  * 多模态（multi-modal）
  * 增加的（incremental）
  * 物理的（physical）
  * 探索的（explore）
  * 社会化的（social）

## 增强学习成功案例

* **十五子棋**
  BM研究中心的Gerarl Tesauro 于1992年使用这种方法来尝试解决十五子棋的问题。从任意初始化的权重weight开始，它通过与自身对弈，训练出了一个评估函数（evaluation function）用于决策，击败了当时的十五子棋冠军。
* **直升机飞控**  
  \(和四旋翼不同，直升机是一个有超过7维控制输入的复杂非线性系统，其自动控制一直被控制界认为是比较困难的问题\)  
  斯坦福的自控直升机通过运用专家演示，微分动态规划，局部模型学习等技术，能够让直升机在空中自主完成一系列姿态动作。

* **四足机器人行走**

  CMU机器人研究所的Matt Zucker用优化和学习模型能够让机器人根据不同的地形状况更好的控制腿部运动。

* **无人车**

  第一辆无人车于1991年在CMU诞生，主要用的是人工神经网络，此作品运用了行为克隆模型，利用数据增强来处理累计误差，是一种在线适应的交互学习方式。

* **打砖块游戏**

  Google Deep Mind使用深度Q学习算法训练出了针对打砖块游戏的agent，四个小时之后完全攻略了这个游戏。

* **Montezuma’s Revenge游戏**

* **第一个基于像素的增强学习应用**

  论文：Evolving large-scale neural networks for vision-based reinforcement learning, 2013, Koutník, Cuccu, Schmidhuber, Gomez

* **围棋 AlphaGo**

  AlphaGo使用的增强学习算法属于蒙特卡罗树搜索，通过学习策略网络和价值网络来修剪搜索树，通过专家示范和大量自我对奕来训练。

---

博客地址：[https://zhuanlan.zhihu.com/p/25065995](https://zhuanlan.zhihu.com/p/25065995)

